{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI4R Workshop 3: Gymnasium and Reinforcement Learning Training"
      ],
      "metadata": {
        "id": "YRqI42yPYgZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity 1: Setting up the Environment"
      ],
      "metadata": {
        "id": "Fhd9F1pvYo2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencies\n",
        "- We will be using `gymnasium` framework as an interface to represent general RL Problems. Dependencies for installation: `swig`\n",
        "- The example for demonstration is the `Lunar Lander` which requires additional dependencies (`Box2D`)\n",
        "- Deep Learning Training Library `stable_baselines3`\n",
        "\n",
        "### Next Steps\n",
        "- [ ] Press `Connect` in the top-right corner of the notebook to connect to a Python Kernel (running on a remote server hosted by Google).\n",
        "- [ ] To run each cell, place the cursor on the cell and press: `shift` + `enter/return`. An easier option is to just\n",
        "- Cells should be marked 'square with a spinning circle' while running, which will go away once the cell execution is complete, and cell outputs will be displayed below (if applicable)\n",
        "- It might take some time for the dependency installation, please be patient and let it continue."
      ],
      "metadata": {
        "id": "fdavgjf0YsJM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkF85pq_YVzx"
      },
      "outputs": [],
      "source": [
        "!pip install swig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2D]"
      ],
      "metadata": {
        "id": "JBkoGJnHZbYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "id": "RrYaFLkiadP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library Imports\n",
        "\n",
        "Let's import the necessary libraries for our activity"
      ],
      "metadata": {
        "id": "uLYYSTJTa8v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO, DDPG, SAC, TD3\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "eFWfkQLDa3MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supress warnings:"
      ],
      "metadata": {
        "id": "SMzoXz-bfxyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"ipykernel.ipkernel\")"
      ],
      "metadata": {
        "id": "xkJkCuMOf0-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity 2: Getting Familiar with Gymnasium Environments"
      ],
      "metadata": {
        "id": "aJwhuJ5bblI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Gymnasium is an API standard for Reinforcement Learning (RL) with an interface that is simple, pythonic, and capable of representing general RL problems.\n",
        "\n",
        "Key functions:\n",
        "- `make()`: Create the environment\n",
        "- `reset()`: Reset the environment to the 'initial state'\n",
        "- `step()`: Compute the transition kernel, as the environment takes a step with a current action\n",
        "- `render()`: Render the environment for visualisation\n",
        "\n",
        "Key details:\n",
        "- `observation_space` : defines the observation space\n",
        "- `action_space`: defines the action space\n",
        "\n",
        "### Let's define our Gymnasium Environment [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "the `make()` function creates the `LunarLander-v2` environment (built-in environment) in `continuous` mode. Other arguments set details such as `gravity`, `wind`, `wind_power`, `turbulence`, `render_mode`, etc.  \n",
        "\n",
        "Other Details:\n",
        "> If `enable_wind=True` is passed, there will be wind effects applied to the lander. The wind is generated using the function `tanh(sin(2 k (t+C)) + sin(pi k (t+C)))`. k is set to 0.01. C is sampled randomly between -9999 and 9999.\n",
        "\n",
        "> `wind_power` dictates the maximum magnitude of linear wind applied to the craft. The recommended value for `wind_power` is between 0.0 and 20.0. `turbulence_power` dictates the maximum magnitude of rotational wind applied to the craft. The recommended value for `turbulence_power` is between 0.0 and 2.0."
      ],
      "metadata": {
        "id": "jAUsMKZ7cnmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\n",
        "    \"LunarLander-v2\",\n",
        "    continuous=True,\n",
        "    gravity=-10.0,\n",
        "    enable_wind=False,\n",
        "    wind_power=15.0,\n",
        "    turbulence_power=1.5,\n",
        "    render_mode = \"rgb_array\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZdfVXxOkbkCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations:\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in `x` & `y`, its linear velocities in `x` & `y`, its `angle`, its `angular velocity`, and `two booleans` that represent whether each leg is in contact with the ground or not. Let's take a look at the observation space for the Lunar Lander environment:"
      ],
      "metadata": {
        "id": "HHBelZfYh70z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the observation space\n",
        "print(\"Observation Space:\", env.observation_space)\n",
        "print(\"Shape:\", env.observation_space.shape)\n",
        "print(\"Sample Observation:\", env.observation_space.sample())"
      ],
      "metadata": {
        "id": "HdtE_IN0fENh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial State\n",
        "\n",
        "Let's **`reset()`** the environment to it's initial state: (`obs` is the observation corresponding to the initial state)"
      ],
      "metadata": {
        "id": "x9TTyaVZdul_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset()\n",
        "print(obs)"
      ],
      "metadata": {
        "id": "VTFX9MELdvOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Render the environment at Initial State:"
      ],
      "metadata": {
        "id": "3vhFdQx-n35w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_current_frame(env):\n",
        "    frame = env.render()    # calls the `render()` function to render the frame\n",
        "    plt.imshow(frame)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "plot_current_frame(env)"
      ],
      "metadata": {
        "id": "55HH1EIvoBJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Action Space\n",
        "\n",
        "- If `continuous=True` is passed, continuous actions (corresponding to the throttle of the engines) will be used and the action space will be `Box(-1, +1, (2,), dtype=np.float32)`.\n",
        "- The first coordinate of an action determines the throttle of the main engine, while the second coordinate specifies the throttle of the lateral boosters.\n",
        "    - Given an action `np.array([main, lateral])`\n",
        "        - the main engine will be turned off completely if main < 0\n",
        "        - the throttle scales affinely from 50% to 100% for `0 <= main <= 1` (in particular, the main engine doesn’t work with less than 50% power).\n",
        "    - Similarly\n",
        "        - if `-0.5 < lateral < 0.5`, the lateral boosters will not fire at all.\n",
        "        - If lateral < -0.5, the left booster will fire, and\n",
        "        - if lateral > 0.5, the right booster will fire.\n",
        "    - Again, the throttle scales affinely from 50% to 100% between -1 and -0.5 (and 0.5 and 1, respectively)."
      ],
      "metadata": {
        "id": "j_30pq4iifrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the action space\n",
        "print(\"Action Space:\", env.action_space)\n",
        "print(\"Shape:\", env.action_space.shape)\n",
        "print(\"Low:\", env.action_space.low)\n",
        "print(\"High:\", env.action_space.high)\n",
        "print(\"Sample Action:\", env.action_space.sample())"
      ],
      "metadata": {
        "id": "SvL74Qn9e67F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward Function / `Step()`:\n",
        "\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "- is increased/decreased the closer/further the lander is to the landing pad.\n",
        "- is increased/decreased the slower/faster the lander is moving.\n",
        "- is decreased the more the lander is tilted (angle not horizontal).\n",
        "- is increased by 10 points for each leg that is in contact with the ground.\n",
        "- is decreased by 0.03 points each frame a side engine is firing.\n",
        "- is decreased by 0.3 points each frame the main engine is firing.\n",
        "- The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "\n",
        "If you are interested you can take a look at how the rewards of the Lunar Lander are calculated inside the [Lunar Lander Class](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/box2d/lunar_lander.py#L638-L662)\n",
        "\n",
        "Let's sample a random action from the action space and take a `step()` within the environment:"
      ],
      "metadata": {
        "id": "2OIcCYvOkog_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random action from the action space:\n",
        "action = env.action_space.sample()\n",
        "print(\"Random action: \", action)\n",
        "\n",
        "# Take a step with the random action:\n",
        "obs, reward, done, terminate, info = env.step(action)\n",
        "\n",
        "print(\"Observation:\", obs)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Done:\", done)\n",
        "print(\"Terminate:\", terminate)\n",
        "print(\"Info:\", info)"
      ],
      "metadata": {
        "id": "ZMGf9qOFixgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's render the environment after the Random Action:"
      ],
      "metadata": {
        "id": "CrfDYGkeoSCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_current_frame(env)"
      ],
      "metadata": {
        "id": "oXuqnplKpTD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a function that takes a random step:"
      ],
      "metadata": {
        "id": "6SPg1XKyo17B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def take_random_step(env):\n",
        "    action = env.action_space.sample()\n",
        "    print(\"Action: {}\".format(action))\n",
        "    obs, reward, done, truncate, info = env.step(action)"
      ],
      "metadata": {
        "id": "-KN7P30_o1fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "take_random_step(env)\n",
        "plot_current_frame(env)"
      ],
      "metadata": {
        "id": "T_Lr2iiCnVM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity 3: Training a Vanilla RL Policy\n",
        "\n",
        "There are [different policies](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html) to choose from.\n",
        "\n",
        "### RL Algorithms\n",
        "\n",
        "This table displays the RL algorithms that are implemented in the Stable Baselines3 project, along with some useful characteristics: support for discrete/continuous actions, multiprocessing.\n",
        "\n",
        "| Name           | Box | Discrete | MultiDiscrete | MultiBinary | Multi Processing |\n",
        "|----------------|-----|----------|---------------|-------------|------------------|\n",
        "| ARS            | ✔️   | ✔️        | ❌             | ❌           | ✔️                |\n",
        "| A2C            | ✔️   | ✔️        | ✔️             | ✔️           | ✔️                |\n",
        "| DDPG           | ✔️   | ❌        | ❌             | ❌           | ✔️                |\n",
        "| DQN            | ❌   | ✔️        | ❌             | ❌           | ✔️                |\n",
        "| HER            | ✔️   | ❌        | ❌             | ❌           | ✔️                |\n",
        "| PPO            | ✔️   | ✔️        | ✔️             | ✔️           | ✔️                |\n",
        "| QR-DQN         | ❌   | ✔️        | ❌             | ❌           | ✔️                |\n",
        "| RecurrentPPO   | ✔️   | ✔️        | ✔️             | ✔️           | ✔️                |\n",
        "| SAC            | ✔️   | ❌        | ❌             | ❌           | ✔️                |\n",
        "| TD3            | ✔️   | ❌        | ❌             | ❌           | ✔️                |\n",
        "| TQC            | ✔️   | ❌        | ❌             | ❌           | ✔️                |\n",
        "| TRPO           | ✔️   | ✔️        | ✔️             | ✔️           | ✔️                |\n",
        "| Maskable PPO   | ✔️   | ✔️        | ✔️             | ✔️           | ✔️                |\n",
        "\n",
        "Since our action space is `continuous` and in `Box` mode, our choice of algorithms can be found from this table above. A2C, DDPG, PPO, SAV, TD3, etc. are some algorithms that we can try."
      ],
      "metadata": {
        "id": "AOMNdI2vrHV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Policy\n",
        "\n",
        "Let's try training a simple policy-based approach to begin with (PPO). First, we need to define the policy:"
      ],
      "metadata": {
        "id": "DEMoYqmVjbFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO(\"MlpPolicy\", env, verbose = 1, learning_rate = 0.0003, n_steps = 2048)"
      ],
      "metadata": {
        "id": "Z_6EybzMopYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code initialises a Proximal Policy Optimisation (PPO) reinforcement learning model using the `MlpPolicy` which is using an MLP (Multi-Layer Perceptron), which is a neural network as the policy network architecture.\n",
        "\n",
        "`verbose = 1` controls the level of verbosity (output information)\n",
        "`learning_rate` sets the learning rate of the network during training.\n",
        "`n_steps = 2048` defines the number of steps the agent will take in the environment before updating the policy. This is also known as the number of steps per epoch."
      ],
      "metadata": {
        "id": "X6pAe6YwG_iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Policy\n",
        "\n",
        "Now, let's train the policy for a total of 100,000 timesteps. With an `n_steps = 2048` that would be at least 49 iterations. Observe that the mean episode length and reward increase with training."
      ],
      "metadata": {
        "id": "S8b1YN79j2UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps = 100000)"
      ],
      "metadata": {
        "id": "11Q-sE27HxhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading/Saving the policy"
      ],
      "metadata": {
        "id": "pnlnSFn_jlBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the trained model"
      ],
      "metadata": {
        "id": "NvLNWaBZQQ-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"ppo_lunarlander\")"
      ],
      "metadata": {
        "id": "Zpw5B1HnQTjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading saved model"
      ],
      "metadata": {
        "id": "7SBqE9kgcfwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO.load(\"ppo_lunarlander\", env = env)"
      ],
      "metadata": {
        "id": "sDhEaNhacfW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Policy\n",
        "\n",
        "Let's evaluate the model and observe the rewards for a single episode:"
      ],
      "metadata": {
        "id": "4eHiPoFbQZqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the environment to it's initial state\n",
        "obs, info = env.reset()\n",
        "\n",
        "# max_steps to run simulation for\n",
        "max_steps = 1000\n",
        "rewards = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "    action, _ = model.predict(obs, deterministic = True)\n",
        "    obs, reward, terminate, truncate, info = env.step(action)\n",
        "    rewards.append(reward)\n",
        "    if terminate or truncate:\n",
        "        break\n",
        "\n",
        "print(f\"Total Reward:{sum(rewards):.2f}\")\n",
        "print(\"Total Number of steps: \", i)\n",
        "print(\"Terminated:\", terminate)"
      ],
      "metadata": {
        "id": "GtchipeeRGmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's wrap this code into a function for future use:"
      ],
      "metadata": {
        "id": "64LdUT_X4CLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_episode(env, model, max_steps=1000):\n",
        "    obs, info = env.reset()\n",
        "    rewards = []\n",
        "\n",
        "    for i in range(max_steps):\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminate, truncate, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        if terminate or truncate:\n",
        "            break\n",
        "\n",
        "    total_reward = sum(rewards)\n",
        "    terminated = terminate\n",
        "\n",
        "    print(f\"Total Reward: {total_reward:.2f}\")\n",
        "    print(\"Total Number of steps:\", i)\n",
        "    print(\"Terminated:\", terminated)\n",
        "\n",
        "    return total_reward, i, terminated"
      ],
      "metadata": {
        "id": "-y-lT3ad4PHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the rewards over timesteps:"
      ],
      "metadata": {
        "id": "k0Sitiw_fEil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting rewards over timesteps\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(rewards, marker='', linestyle='-', color='b')\n",
        "plt.title('Rewards Over Timesteps')\n",
        "plt.xlabel('Timestep')\n",
        "plt.ylabel('Reward')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ryOg_aHgfDpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualising the Episode"
      ],
      "metadata": {
        "id": "vzkxjjW8hrz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to Simulate and Render Episode Frames"
      ],
      "metadata": {
        "id": "LJ4fDWqtkSxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Simulate and Render Episode Frames\n",
        "def simulate_and_render(env, model, max_steps = 1000):\n",
        "    # Reset environment to initial state\n",
        "    obs, info = env.reset()\n",
        "    frames = []\n",
        "    terminate = truncate = False\n",
        "    for _ in range(max_steps):\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminate, truncate, info = env.step(action)\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        if terminate or truncate:\n",
        "            break\n",
        "    return frames"
      ],
      "metadata": {
        "id": "9d_-s5J7O4FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to Animate Frames to Display in Notebook"
      ],
      "metadata": {
        "id": "al_0uMeSkQ73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Animate Frames to Display in Notebook\n",
        "def animate_episode_frames(frames, interval=50):\n",
        "    fig, ax = plt.subplots()\n",
        "    patch = ax.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    plt.close()\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, animate, frames=len(frames), interval=interval)\n",
        "\n",
        "    return HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "3DEJw5EMkILG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting it together:"
      ],
      "metadata": {
        "id": "s6czRUx3kTz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frames = simulate_and_render(env, model)\n",
        "animate_episode_frames(frames)"
      ],
      "metadata": {
        "id": "RiPFIi1kkUr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The cell above might be responsible for increasing the notebook size. If that becomes an issue, you can just clear the output of the cell*"
      ],
      "metadata": {
        "id": "GIBzIX8Qkr96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity 4: Customising the Gym Environment (Observations, Actions, Rewards)"
      ],
      "metadata": {
        "id": "9izm8fYPlG7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Observations\n",
        "\n",
        "Gym supports the use of `ObservationWrapper` to modify the observation space and create Custom Observations.\n",
        "\n",
        "Observation wrappers are useful if you want to apply some function to the observations that are returned by an environment. If you implement an observation wrapper, you only need to define this transformation by implementing the `gymnasium.ObservationWrapper.observation()` method. Moreover, you should remember to update the observation space, if the transformation changes the shape of observations (e.g. by transforming dictionaries into numpy arrays, as in the following example)."
      ],
      "metadata": {
        "id": "Qi2hryBgOswb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following values are available to us -\n",
        "- observation scale factors\n",
        "- lander mass\n",
        "- value of g\n",
        "\n",
        "Let's create a custom observation to demonstrate this. The custom observation we choose to add is the Potential Energy of the Lander.\n",
        "\n",
        "$Potential Energy = m * g * h$"
      ],
      "metadata": {
        "id": "emR6wTzXVfSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scale_factors = np.array([10, 6.666, 5, 7.5, 1, 2.5, 1, 1], dtype=np.float32)\n",
        "lander_mass = 4.82 # kg\n",
        "g = 10.0 # m/s^2"
      ],
      "metadata": {
        "id": "Y6oTO-JblGeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the custom observation wrapper"
      ],
      "metadata": {
        "id": "29sxpsn7PqdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomObservationWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CustomObservationWrapper, self).__init__(env)\n",
        "        # Get the old observation space\n",
        "        old_space = env.observation_space\n",
        "\n",
        "        # Define new observation space bounds\n",
        "        low = np.concatenate([old_space.low, [0]])\n",
        "        high = np.concatenate([old_space.high, [np.inf]])\n",
        "        # Define new observation space\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=low, high=high, dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        # Extract and scale original observations\n",
        "        x, y, vx, vy, theta, omega, leg1, leg2 = obs * scale_factors\n",
        "\n",
        "        # Calculate potential energy\n",
        "        PE = lander_mass * g * y\n",
        "\n",
        "        return np.append(obs, PE)"
      ],
      "metadata": {
        "id": "64jvFRo8lGbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to wrap the environment with the custom observation wrapper:"
      ],
      "metadata": {
        "id": "5emkzMd9Pw60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# env = CustomObservationWrapper(env)"
      ],
      "metadata": {
        "id": "NK8duhfKlGYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise\n",
        "\n",
        "Modify the `CustomObservationWrapper` to include two additional observations:\n",
        "\n",
        "- Kinetic Energy Along x-axis\n",
        "- Kinetic Energy Along y-axis\n",
        "\n",
        "*Hints:*\n",
        "- The velocity along x and y axis is available in the original observations\n",
        "- $Kinetic Energy = 1/2 * mass * (velocity)^2$"
      ],
      "metadata": {
        "id": "SFxmnTt4QFcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomObservationWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CustomObservationWrapper, self).__init__(env)\n",
        "        # Get the old observation space\n",
        "        old_space = env.observation_space\n",
        "\n",
        "        # Define new observation space bounds\n",
        "        low = np.concatenate([old_space.low, [0]])          # Modify HERE\n",
        "        high = np.concatenate([old_space.high, [np.inf]])   # Modify HERE\n",
        "        # Define new observation space\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=low, high=high, dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        # Extract and scale original observations\n",
        "        x, y, vx, vy, theta, omega, leg1, leg2 = obs * scale_factors\n",
        "\n",
        "        # Calculate potential energy\n",
        "        PE = lander_mass * g * y\n",
        "\n",
        "        # Calculate Kinetic Energies HERE\n",
        "\n",
        "        return np.append(obs, PE)"
      ],
      "metadata": {
        "id": "Wc7pM5NGlGVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# env = CustomObservationWrapper(env)"
      ],
      "metadata": {
        "id": "dDyjxhE9lGTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Actions\n",
        "\n",
        "*Do you remember the action space?*\n",
        "\n",
        "### Action Space\n",
        "\n",
        "- If `continuous=True` is passed, continuous actions (corresponding to the throttle of the engines) will be used and the action space will be `Box(-1, +1, (2,), dtype=np.float32)`.\n",
        "- The first coordinate of an action determines the throttle of the main engine, while the second coordinate specifies the throttle of the lateral boosters.\n",
        "    - Given an action `np.array([main, lateral])`\n",
        "        - the main engine will be turned off completely if main < 0\n",
        "        - the throttle scales affinely from 50% to 100% for `0 <= main <= 1` (in particular, the main engine doesn’t work with less than 50% power).\n",
        "    - Similarly\n",
        "        - if `-0.5 < lateral < 0.5`, the lateral boosters will not fire at all.\n",
        "        - If lateral < -0.5, the left booster will fire, and\n",
        "        - if lateral > 0.5, the right booster will fire.\n",
        "    - Again, the throttle scales affinely from 50% to 100% between -1 and -0.5 (and 0.5 and 1, respectively)."
      ],
      "metadata": {
        "id": "C_kSGwLxREg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's Simplify The Action Space!**\n",
        "\n",
        "- Define 3 Action variables corresponding to the 3 boosters - values will range from 0 to 1\n",
        "- Modified throttle control:\n",
        "    - throttle 0 means idle\n",
        "    - throttle > 0 will start from 50% with throttle 1 will equal 100%"
      ],
      "metadata": {
        "id": "MCrUxhn-RjdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomActionWrapper(gym.ActionWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        # Define new action space with 3 actions ranging from 0 to 1\n",
        "        self.action_space = gym.spaces.Box(\n",
        "            low = 0, high = 1, shape = (3,), dtype = np.float32\n",
        "        )\n",
        "\n",
        "    def action(self, action):\n",
        "        # Extract the three acitons\n",
        "        main, left, right = action\n",
        "\n",
        "        # Calculate the main engine throttle\n",
        "        main_engine = -1 if main == 0 else (main/2 + 0.5)\n",
        "\n",
        "        # Calculate the lateral engine throttles\n",
        "        left_engine = (-1)*left/2 + 0.5\n",
        "        right_engine = right/2 + 0.5\n",
        "\n",
        "        lateral = left_engine + right_engine\n",
        "        return np.array([main_engine, lateral])\n"
      ],
      "metadata": {
        "id": "qpi6tU7zRDab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise:\n",
        "\n",
        "Some issues with this action space:\n",
        "\n",
        "- What to do if both left/right thrusters have positive throttle? -> Fire the one with higher value\n",
        "- What if they have the same throttle? -> Fire None\n",
        "- If only 0 means idle then the throttle might be active most of the time for the rockets, let's set a lower cutoff like 0.25 instead to discourage using of thrusters.\n",
        "\n",
        "This relationship can be expressed by the following equation: $ y = 2/3 (x-1) + 1$\n",
        "\n",
        "Let's fix the action wrapper accordingly!"
      ],
      "metadata": {
        "id": "Hf_JxI3XV5HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomActionWrapper(gym.ActionWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        # Define new action space with 3 actions ranging from 0 to 1\n",
        "        self.action_space = gym.spaces.Box(\n",
        "            low = 0, high = 1, shape = (3,), dtype = np.float32\n",
        "        )\n",
        "\n",
        "    def action(self, action):\n",
        "        # Extract the three acitons\n",
        "        main, left, right = action\n",
        "\n",
        "        # Calculate the main engine throttle\n",
        "        main_engine = -1 if main == 0 else (main/2 + 0.5)\n",
        "\n",
        "        # Make changes around HERE\n",
        "        # Calculate the lateral engine throttles\n",
        "        left_engine = (-1)*left/2 + 0.5\n",
        "        right_engine = right/2 + 0.5\n",
        "        lateral = left_engine + right_engine\n",
        "\n",
        "        return np.array([main_engine, lateral])"
      ],
      "metadata": {
        "id": "uRHscLKPRDXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Rewards\n",
        "\n",
        "Reward wrappers are used to transform the reward that is returned by an environment. As for the previous wrappers, you need to specify that transformation by implementing the `gymnasium.RewardWrapper.reward()` method. Also, you might want to update the reward range of the wrapper.\n",
        "\n",
        "The following is an example of how to use the `RewardWrapper` class for custom rewards:"
      ],
      "metadata": {
        "id": "WlKKCvY7h77n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomReward(gym.RewardWrapper):\n",
        "    def __init__(self, env, min_reward, max_reward):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        # Make modifications to the reward function here\n",
        "        return reward"
      ],
      "metadata": {
        "id": "Whm-GI9DRDUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try something different now and use the `gym.Wrapper` class to reproduce the existing rewards so that we can modify them:"
      ],
      "metadata": {
        "id": "xhW-UDD6uBDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.prev_shaping = None\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, terminate, truncate, info = self.env.step(action)  # take a step using the original environment\n",
        "\n",
        "        x, y, vx, vy, theta, omega, leg1, leg2 = \\\n",
        "            state[0], state[1], state[2], state[3], state[4], state[5], state[6], state[7]\n",
        "\n",
        "        shaping = (\n",
        "            -100 * np.sqrt(x*x + y*y)\n",
        "            - 100 * np.sqrt(vx*vx + vy*vy)\n",
        "            - 100 * abs(theta)\n",
        "            + 10 * leg1\n",
        "            + 10 * leg2\n",
        "        )\n",
        "\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        # calculate engine powers from action\n",
        "        m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5\n",
        "        direction = np.sign(action[1])\n",
        "        s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "\n",
        "        # Subtract power costs from the reward\n",
        "        reward -= (m_power * 0.30)  # Main engine power penalty\n",
        "        reward -= (s_power * 0.03)  # Side engine power penalty\n",
        "\n",
        "        # Check for termination conditions\n",
        "        if self.unwrapped.game_over or abs(state[0]) >= 1.0:\n",
        "            reward = -100\n",
        "        if not self.unwrapped.lander.awake:\n",
        "            reward = +100\n",
        "\n",
        "        return state, reward, terminate, truncate, info"
      ],
      "metadata": {
        "id": "lcKGal4Yu8Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We can also achieve the same using the `gym.RewardWrapper`. Showing `gym.Wrapper` class as an example*"
      ],
      "metadata": {
        "id": "kFIxerPix_0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise\n",
        "\n",
        "Make the following modifications to the reward function -\n",
        "\n",
        "- [ ] Change the Reward shaping: Introduce a Higher Penalty for deviating from the center\n",
        "- [ ] Add another penalty to avoid having high values of Angular Velocity\n",
        "\n",
        "Think of at least one other Custom Reward and implement it into the Custom Reward Wrapper."
      ],
      "metadata": {
        "id": "5neNkogZiAk7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F07YwNwulGQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting Everything Together\n",
        "\n",
        "Now that we have multiple wrappers to our environment, let's put them together. We can just simply wrap each environment on top of each other like this:"
      ],
      "metadata": {
        "id": "RKl582zo0GVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\n",
        "    \"LunarLander-v2\",\n",
        "    continuous=True,\n",
        "    gravity=-10.0,\n",
        "    enable_wind=False,\n",
        "    wind_power=15.0,\n",
        "    turbulence_power=1.5,\n",
        "    render_mode = \"rgb_array\"\n",
        ")\n",
        "\n",
        "env = CustomObservationWrapper(env)\n",
        "env = CustomActionWrapper(env)\n",
        "env = CustomRewardWrapper(env)"
      ],
      "metadata": {
        "id": "VQZmXZaQ0JYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly check our new wrapped environment now to make sure everything is in order:"
      ],
      "metadata": {
        "id": "EfjIKa030ihQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Observation Space: \", env.observation_space)\n",
        "print(\"Action Space: \", env.action_space)\n",
        "\n",
        "# Reset environment\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Take a random action and get the reward\n",
        "random_action = env.action_space.sample()\n",
        "obs, reward, terminate, truncate, info = env.step(random_action)\n",
        "print(\"Reward: \", reward)"
      ],
      "metadata": {
        "id": "V6tgprrP0mpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different Policies\n",
        "\n",
        "We can easily swap out policies in a plug-and-play manner with the `stable_baselines3` RL Library. Following are a few examples. Feel free to try them out:"
      ],
      "metadata": {
        "id": "BjKL_3DUiCsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = DDPG(\"MlpPolicy\", env, verbose=1)\n",
        "#model = SAC(\"MlpPolicy\", env, verbose=1)\n",
        "#model = TD3(\"MlpPolicy\", env, verbose=1)\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)"
      ],
      "metadata": {
        "id": "iPZKYXt2kXRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train & Save Model"
      ],
      "metadata": {
        "id": "8__dm5Jl4SgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=100000)\n",
        "model.save(\"lunarlander_policy_custom\")"
      ],
      "metadata": {
        "id": "0MnZRM-T3yRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate:"
      ],
      "metadata": {
        "id": "UmoNAygs3-bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_episode(env, model)"
      ],
      "metadata": {
        "id": "18CqKqo15XYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = simulate_and_render(env, model)\n",
        "animate_episode_frames(frames)"
      ],
      "metadata": {
        "id": "StypDi7V5aKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lkqkWCjt5Vpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise\n",
        "\n",
        "- Swap out the existing policy for a different policy than PPO. Train your environment for `100000` steps again and evaluate/see the results.\n",
        "\n",
        "Try on your own time (might take a while to run):\n",
        "- Make modifications to the environment\n",
        "- For each modification, train for a small number of iterations (100,000) to observe that it doesn't negatively impact the policy behavior\n",
        "- Train your choice of policy for 1 Million timesteps and observe the results (don't forget to save and download your policy for future use!)"
      ],
      "metadata": {
        "id": "8qodZFPXihu4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p8yKUek8ijKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SqTs70kO5KP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "arH6s-vh5KLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTpsgxwl5KHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity 5: Interdiscilinery Reflection"
      ],
      "metadata": {
        "id": "s-76qRFmiJK5"
      }
    }
  ]
}